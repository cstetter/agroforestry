{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into dataframe\n",
    "df = pd.read_csv('data_field/ae_input_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the data \n",
    "# list(df.columns[~df.columns.str.contains('2018')])\n",
    "# Variables from day 2\n",
    "lstX = [\n",
    "    'cc_temp_springsum',\n",
    "    'cc_temp_fallwint',\n",
    "    'cc_temp_volatility',\n",
    "    'cc_precip_quant_springsum',\n",
    "    'cc_precip_quant_fallwint',\n",
    "    'cc_no_rain',\n",
    "    'cc_heavy_rain',\n",
    "    'cc_hot_days',\n",
    "    'cc_frost_days',\n",
    "    'cc_storms',\n",
    "    'cc_hail',\n",
    " ]\n",
    "\n",
    "X_all = df.loc[:,lstX]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split & normalize data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw = train_test_split(X_all, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax scale data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalerX = MinMaxScaler()\n",
    "X_train = scalerX.fit_transform(X_train_raw)\n",
    "X_test = scalerX.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load keras and tesorflow \n",
    "import tensorflow as tf\n",
    "import keras as ke\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Dim is equal to: 5\n",
      "Input Dim is equal to: 11\n"
     ]
    }
   ],
   "source": [
    "## Setup Autoencode as an Keras model\n",
    "# This example is adopted based on: https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "# Set dimensionality of the encoding space. \n",
    "# As in the PCA example we want a 8 dim encoding space.\n",
    "encoding_dim = 5\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "print('Encoding Dim is equal to:', encoding_dim)\n",
    "print('Input Dim is equal to:', input_dim)\n",
    "\n",
    "# In Keras we can specify a NN layer in one line of code. This is what \n",
    "# we use here to specify an...\n",
    "\n",
    "# 1) input layer that has the dimension equal to the number of variables\n",
    "#    in the input (here =28)\n",
    "input_dat = Input(shape=(input_dim,))\n",
    "\n",
    "# 2) the second layer is the encoding layer. It takes the output from the\n",
    "#    input layer (\"input_dat\") as input and is a \"dense\" layer with \n",
    "#    \"encoding_dim\" (=8) neurons. It uses the \"relu\" as activation \n",
    "encoded = Dense(encoding_dim, activation='relu')(input_dat)\n",
    "\n",
    "# 3) The third layer is the decoding layer, which is our output layer. \n",
    "#    It takes the output from the encoding layer as input (\"encoded\"). It has \n",
    "#    \"input_dim\"=28 neurons. The acitvation function is not specified here, which \n",
    "#   means that we use the default activation function [identity function]. \n",
    "decoded = Dense(input_dim)(encoded) \n",
    "\n",
    "# Using these layer we build a NN in tensorflow. We do this be passing the \n",
    "# input layer and the last layer to the keras \"Model\" function. \n",
    "# This is sufficient for Keras to now how to build the complete model. \n",
    "# (The information how the hidden layers should look like, is know because \n",
    "# we passed this as input to the output layer)\n",
    "autoencoder = Model(input_dat, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we tell Keras/tensorflow which optimization algorithm we want to use\n",
    "# We also need to define a learning rate. This might not be the optimal choice\n",
    "# here. It we could tune this parameter to obtain better results\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate)\n",
    "\n",
    "# We also need to define the type of loss function we would to considere. \n",
    "# Since we have a regression task we use MSE. \n",
    "# With this we can now compile the model. \n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 168 samples, validate on 42 samples\n",
      "Epoch 1/300\n",
      "168/168 [==============================] - 0s 558us/step - loss: 3.8507 - val_loss: 4.0338\n",
      "Epoch 2/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 3.7980 - val_loss: 3.9477\n",
      "Epoch 3/300\n",
      "168/168 [==============================] - 0s 39us/step - loss: 3.7053 - val_loss: 3.9149\n",
      "Epoch 4/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 3.6135 - val_loss: 3.8375\n",
      "Epoch 5/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 3.5298 - val_loss: 3.7881\n",
      "Epoch 6/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 3.4623 - val_loss: 3.7624\n",
      "Epoch 7/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 3.2938 - val_loss: 3.6731\n",
      "Epoch 8/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 3.2255 - val_loss: 3.5965\n",
      "Epoch 9/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 3.1372 - val_loss: 3.4779\n",
      "Epoch 10/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 3.0800 - val_loss: 3.3961\n",
      "Epoch 11/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 3.0311 - val_loss: 3.3304\n",
      "Epoch 12/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.9552 - val_loss: 3.2544\n",
      "Epoch 13/300\n",
      "168/168 [==============================] - 0s 87us/step - loss: 2.8896 - val_loss: 3.1859\n",
      "Epoch 14/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.8409 - val_loss: 3.0761\n",
      "Epoch 15/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.8114 - val_loss: 2.9986\n",
      "Epoch 16/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.7436 - val_loss: 2.9716\n",
      "Epoch 17/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.7087 - val_loss: 2.8982\n",
      "Epoch 18/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.6563 - val_loss: 2.8405\n",
      "Epoch 19/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 2.6018 - val_loss: 2.7964\n",
      "Epoch 20/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.5663 - val_loss: 2.7743\n",
      "Epoch 21/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.5019 - val_loss: 2.7420\n",
      "Epoch 22/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.4540 - val_loss: 2.6938\n",
      "Epoch 23/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.4144 - val_loss: 2.6558\n",
      "Epoch 24/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 2.3609 - val_loss: 2.6012\n",
      "Epoch 25/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.3047 - val_loss: 2.5582\n",
      "Epoch 26/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.2726 - val_loss: 2.5260\n",
      "Epoch 27/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.2171 - val_loss: 2.5052\n",
      "Epoch 28/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 2.1866 - val_loss: 2.4560\n",
      "Epoch 29/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.1647 - val_loss: 2.4239\n",
      "Epoch 30/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.1419 - val_loss: 2.3909\n",
      "Epoch 31/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.1223 - val_loss: 2.3785\n",
      "Epoch 32/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 2.1085 - val_loss: 2.3393\n",
      "Epoch 33/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.0824 - val_loss: 2.3076\n",
      "Epoch 34/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.0543 - val_loss: 2.2656\n",
      "Epoch 35/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.0284 - val_loss: 2.2399\n",
      "Epoch 36/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 2.0018 - val_loss: 2.2140\n",
      "Epoch 37/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.9836 - val_loss: 2.2040\n",
      "Epoch 38/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.9734 - val_loss: 2.1736\n",
      "Epoch 39/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.9515 - val_loss: 2.1645\n",
      "Epoch 40/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.9334 - val_loss: 2.1568\n",
      "Epoch 41/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.9218 - val_loss: 2.1109\n",
      "Epoch 42/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.9077 - val_loss: 2.0968\n",
      "Epoch 43/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.8962 - val_loss: 2.0675\n",
      "Epoch 44/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.8790 - val_loss: 2.0578\n",
      "Epoch 45/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.8649 - val_loss: 2.0369\n",
      "Epoch 46/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.8496 - val_loss: 2.0028\n",
      "Epoch 47/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.8427 - val_loss: 1.9904\n",
      "Epoch 48/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.8321 - val_loss: 1.9828\n",
      "Epoch 49/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.8222 - val_loss: 1.9617\n",
      "Epoch 50/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.8069 - val_loss: 1.9550\n",
      "Epoch 51/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.7714 - val_loss: 1.9127\n",
      "Epoch 52/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.7562 - val_loss: 1.8845\n",
      "Epoch 53/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.7433 - val_loss: 1.8744\n",
      "Epoch 54/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.7368 - val_loss: 1.8387\n",
      "Epoch 55/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.7274 - val_loss: 1.8299\n",
      "Epoch 56/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.7079 - val_loss: 1.8223\n",
      "Epoch 57/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.6922 - val_loss: 1.7981\n",
      "Epoch 58/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.6640 - val_loss: 1.7650\n",
      "Epoch 59/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.6517 - val_loss: 1.7323\n",
      "Epoch 60/300\n",
      "168/168 [==============================] - 0s 39us/step - loss: 1.6437 - val_loss: 1.7242\n",
      "Epoch 61/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.6346 - val_loss: 1.7033\n",
      "Epoch 62/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.6131 - val_loss: 1.6963\n",
      "Epoch 63/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.5934 - val_loss: 1.6907\n",
      "Epoch 64/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.5828 - val_loss: 1.6859\n",
      "Epoch 65/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.5737 - val_loss: 1.6604\n",
      "Epoch 66/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.5555 - val_loss: 1.6545\n",
      "Epoch 67/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.5499 - val_loss: 1.6502\n",
      "Epoch 68/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.5425 - val_loss: 1.6466\n",
      "Epoch 69/300\n",
      "168/168 [==============================] - 0s 88us/step - loss: 1.5318 - val_loss: 1.6436\n",
      "Epoch 70/300\n",
      "168/168 [==============================] - 0s 12us/step - loss: 1.5232 - val_loss: 1.6411\n",
      "Epoch 71/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.5147 - val_loss: 1.6382\n",
      "Epoch 72/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.5053 - val_loss: 1.6072\n",
      "Epoch 73/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4969 - val_loss: 1.6014\n",
      "Epoch 74/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4934 - val_loss: 1.5909\n",
      "Epoch 75/300\n",
      "168/168 [==============================] - 0s 18us/step - loss: 1.4905 - val_loss: 1.5802\n",
      "Epoch 76/300\n",
      "168/168 [==============================] - 0s 12us/step - loss: 1.4848 - val_loss: 1.5764\n",
      "Epoch 77/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4770 - val_loss: 1.5733\n",
      "Epoch 78/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4743 - val_loss: 1.5707\n",
      "Epoch 79/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4694 - val_loss: 1.5681\n",
      "Epoch 80/300\n",
      "168/168 [==============================] - 0s 6us/step - loss: 1.4628 - val_loss: 1.5653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4548 - val_loss: 1.5628\n",
      "Epoch 82/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4459 - val_loss: 1.5552\n",
      "Epoch 83/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4409 - val_loss: 1.5513\n",
      "Epoch 84/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4384 - val_loss: 1.5489\n",
      "Epoch 85/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.4364 - val_loss: 1.5399\n",
      "Epoch 86/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4331 - val_loss: 1.5304\n",
      "Epoch 87/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4313 - val_loss: 1.5279\n",
      "Epoch 88/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4246 - val_loss: 1.5258\n",
      "Epoch 89/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4214 - val_loss: 1.5177\n",
      "Epoch 90/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4182 - val_loss: 1.5151\n",
      "Epoch 91/300\n",
      "168/168 [==============================] - 0s 6us/step - loss: 1.4130 - val_loss: 1.5069\n",
      "Epoch 92/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4114 - val_loss: 1.5044\n",
      "Epoch 93/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4101 - val_loss: 1.5026\n",
      "Epoch 94/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4061 - val_loss: 1.5010\n",
      "Epoch 95/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4031 - val_loss: 1.4995\n",
      "Epoch 96/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4046 - val_loss: 1.4866\n",
      "Epoch 97/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.4034 - val_loss: 1.4642\n",
      "Epoch 98/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.4023 - val_loss: 1.4609\n",
      "Epoch 99/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3964 - val_loss: 1.4582\n",
      "Epoch 100/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3949 - val_loss: 1.4561\n",
      "Epoch 101/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3937 - val_loss: 1.4543\n",
      "Epoch 102/300\n",
      "168/168 [==============================] - 0s 83us/step - loss: 1.3927 - val_loss: 1.4528\n",
      "Epoch 103/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3918 - val_loss: 1.4514\n",
      "Epoch 104/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3909 - val_loss: 1.4502\n",
      "Epoch 105/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3901 - val_loss: 1.4368\n",
      "Epoch 106/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3893 - val_loss: 1.4343\n",
      "Epoch 107/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3885 - val_loss: 1.4327\n",
      "Epoch 108/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3878 - val_loss: 1.4313\n",
      "Epoch 109/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3871 - val_loss: 1.4253\n",
      "Epoch 110/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3835 - val_loss: 1.4221\n",
      "Epoch 111/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3823 - val_loss: 1.4206\n",
      "Epoch 112/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3814 - val_loss: 1.4194\n",
      "Epoch 113/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.3805 - val_loss: 1.4183\n",
      "Epoch 114/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3798 - val_loss: 1.4173\n",
      "Epoch 115/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3790 - val_loss: 1.4163\n",
      "Epoch 116/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3783 - val_loss: 1.4155\n",
      "Epoch 117/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3776 - val_loss: 1.4147\n",
      "Epoch 118/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3754 - val_loss: 1.4139\n",
      "Epoch 119/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3746 - val_loss: 1.4131\n",
      "Epoch 120/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3739 - val_loss: 1.4124\n",
      "Epoch 121/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3732 - val_loss: 1.4117\n",
      "Epoch 122/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3726 - val_loss: 1.4110\n",
      "Epoch 123/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3720 - val_loss: 1.4103\n",
      "Epoch 124/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3714 - val_loss: 1.4097\n",
      "Epoch 125/300\n",
      "168/168 [==============================] - 0s 6us/step - loss: 1.3708 - val_loss: 1.4091\n",
      "Epoch 126/300\n",
      "168/168 [==============================] - 0s 24us/step - loss: 1.3703 - val_loss: 1.4022\n",
      "Epoch 127/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3697 - val_loss: 1.4009\n",
      "Epoch 128/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3608 - val_loss: 1.3975\n",
      "Epoch 129/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3493 - val_loss: 1.3950\n",
      "Epoch 130/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3439 - val_loss: 1.3932\n",
      "Epoch 131/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3318 - val_loss: 1.3841\n",
      "Epoch 132/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.3213 - val_loss: 1.3813\n",
      "Epoch 133/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2917 - val_loss: 1.3788\n",
      "Epoch 134/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2855 - val_loss: 1.3766\n",
      "Epoch 135/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2786 - val_loss: 1.3526\n",
      "Epoch 136/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2744 - val_loss: 1.3293\n",
      "Epoch 137/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.2728 - val_loss: 1.3243\n",
      "Epoch 138/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2695 - val_loss: 1.3214\n",
      "Epoch 139/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2679 - val_loss: 1.3193\n",
      "Epoch 140/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2646 - val_loss: 1.3176\n",
      "Epoch 141/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2634 - val_loss: 1.3162\n",
      "Epoch 142/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.2594 - val_loss: 1.3149\n",
      "Epoch 143/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2530 - val_loss: 1.3079\n",
      "Epoch 144/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2485 - val_loss: 1.3057\n",
      "Epoch 145/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2437 - val_loss: 1.3045\n",
      "Epoch 146/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2389 - val_loss: 1.3035\n",
      "Epoch 147/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2378 - val_loss: 1.3027\n",
      "Epoch 148/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.2355 - val_loss: 1.3020\n",
      "Epoch 149/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2377 - val_loss: 1.3014\n",
      "Epoch 150/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2354 - val_loss: 1.3008\n",
      "Epoch 151/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2345 - val_loss: 1.3003\n",
      "Epoch 152/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2305 - val_loss: 1.2999\n",
      "Epoch 153/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2280 - val_loss: 1.2995\n",
      "Epoch 154/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.2271 - val_loss: 1.2991\n",
      "Epoch 155/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2265 - val_loss: 1.2987\n",
      "Epoch 156/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2259 - val_loss: 1.2984\n",
      "Epoch 157/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2254 - val_loss: 1.2981\n",
      "Epoch 158/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2249 - val_loss: 1.2978\n",
      "Epoch 159/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2245 - val_loss: 1.2975\n",
      "Epoch 160/300\n",
      "168/168 [==============================] - 0s 13us/step - loss: 1.2241 - val_loss: 1.2972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/300\n",
      "168/168 [==============================] - 0s 24us/step - loss: 1.2237 - val_loss: 1.2970\n",
      "Epoch 162/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2233 - val_loss: 1.2967\n",
      "Epoch 163/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2229 - val_loss: 1.2965\n",
      "Epoch 164/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2226 - val_loss: 1.2963\n",
      "Epoch 165/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2207 - val_loss: 1.2961\n",
      "Epoch 166/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2202 - val_loss: 1.2958\n",
      "Epoch 167/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2198 - val_loss: 1.2956\n",
      "Epoch 168/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2194 - val_loss: 1.2953\n",
      "Epoch 169/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2190 - val_loss: 1.2951\n",
      "Epoch 170/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2187 - val_loss: 1.2948\n",
      "Epoch 171/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2183 - val_loss: 1.2946\n",
      "Epoch 172/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2180 - val_loss: 1.2944\n",
      "Epoch 173/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2176 - val_loss: 1.2941\n",
      "Epoch 174/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2173 - val_loss: 1.2939\n",
      "Epoch 175/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2170 - val_loss: 1.2936\n",
      "Epoch 176/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2167 - val_loss: 1.2933\n",
      "Epoch 177/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2151 - val_loss: 1.2931\n",
      "Epoch 178/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.2131 - val_loss: 1.2928\n",
      "Epoch 179/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2122 - val_loss: 1.2925\n",
      "Epoch 180/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2117 - val_loss: 1.2923\n",
      "Epoch 181/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2113 - val_loss: 1.2920\n",
      "Epoch 182/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2109 - val_loss: 1.2918\n",
      "Epoch 183/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2106 - val_loss: 1.2916\n",
      "Epoch 184/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.2102 - val_loss: 1.2913\n",
      "Epoch 185/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2099 - val_loss: 1.2911\n",
      "Epoch 186/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2096 - val_loss: 1.2909\n",
      "Epoch 187/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2093 - val_loss: 1.2906\n",
      "Epoch 188/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2090 - val_loss: 1.2904\n",
      "Epoch 189/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.2087 - val_loss: 1.2902\n",
      "Epoch 190/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2084 - val_loss: 1.2900\n",
      "Epoch 191/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2082 - val_loss: 1.2898\n",
      "Epoch 192/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2079 - val_loss: 1.2895\n",
      "Epoch 193/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2076 - val_loss: 1.2893\n",
      "Epoch 194/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.2074 - val_loss: 1.2837\n",
      "Epoch 195/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2071 - val_loss: 1.2827\n",
      "Epoch 196/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2069 - val_loss: 1.2821\n",
      "Epoch 197/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2066 - val_loss: 1.2817\n",
      "Epoch 198/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2064 - val_loss: 1.2814\n",
      "Epoch 199/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2061 - val_loss: 1.2810\n",
      "Epoch 200/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2059 - val_loss: 1.2807\n",
      "Epoch 201/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.2056 - val_loss: 1.2805\n",
      "Epoch 202/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2054 - val_loss: 1.2802\n",
      "Epoch 203/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2052 - val_loss: 1.2800\n",
      "Epoch 204/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2049 - val_loss: 1.2798\n",
      "Epoch 205/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2047 - val_loss: 1.2796\n",
      "Epoch 206/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.2045 - val_loss: 1.2794\n",
      "Epoch 207/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2042 - val_loss: 1.2792\n",
      "Epoch 208/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2040 - val_loss: 1.2791\n",
      "Epoch 209/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2038 - val_loss: 1.2789\n",
      "Epoch 210/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2036 - val_loss: 1.2787\n",
      "Epoch 211/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2034 - val_loss: 1.2786\n",
      "Epoch 212/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2031 - val_loss: 1.2784\n",
      "Epoch 213/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2029 - val_loss: 1.2782\n",
      "Epoch 214/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2027 - val_loss: 1.2781\n",
      "Epoch 215/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2025 - val_loss: 1.2779\n",
      "Epoch 216/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2023 - val_loss: 1.2777\n",
      "Epoch 217/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.2021 - val_loss: 1.2776\n",
      "Epoch 218/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2019 - val_loss: 1.2774\n",
      "Epoch 219/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2017 - val_loss: 1.2773\n",
      "Epoch 220/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2015 - val_loss: 1.2772\n",
      "Epoch 221/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2013 - val_loss: 1.2770\n",
      "Epoch 222/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2011 - val_loss: 1.2769\n",
      "Epoch 223/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2009 - val_loss: 1.2768\n",
      "Epoch 224/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2007 - val_loss: 1.2767\n",
      "Epoch 225/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2005 - val_loss: 1.2766\n",
      "Epoch 226/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.2003 - val_loss: 1.2765\n",
      "Epoch 227/300\n",
      "168/168 [==============================] - 0s 107us/step - loss: 1.2001 - val_loss: 1.2765\n",
      "Epoch 228/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1999 - val_loss: 1.2764\n",
      "Epoch 229/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1998 - val_loss: 1.2763\n",
      "Epoch 230/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1996 - val_loss: 1.2763\n",
      "Epoch 231/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1994 - val_loss: 1.2763\n",
      "Epoch 232/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1992 - val_loss: 1.2763\n",
      "Epoch 233/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1990 - val_loss: 1.2764\n",
      "Epoch 234/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.1989 - val_loss: 1.2766\n",
      "Epoch 235/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1987 - val_loss: 1.2772\n",
      "Epoch 236/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1985 - val_loss: 1.2829\n",
      "Epoch 237/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1984 - val_loss: 1.2828\n",
      "Epoch 238/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1982 - val_loss: 1.2827\n",
      "Epoch 239/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1969 - val_loss: 1.2826\n",
      "Epoch 240/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1962 - val_loss: 1.2825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1959 - val_loss: 1.2824\n",
      "Epoch 242/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1957 - val_loss: 1.2824\n",
      "Epoch 243/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1928 - val_loss: 1.2823\n",
      "Epoch 244/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1919 - val_loss: 1.2822\n",
      "Epoch 245/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1915 - val_loss: 1.2820\n",
      "Epoch 246/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1912 - val_loss: 1.2819\n",
      "Epoch 247/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1910 - val_loss: 1.2818\n",
      "Epoch 248/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1908 - val_loss: 1.2817\n",
      "Epoch 249/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.1906 - val_loss: 1.2816\n",
      "Epoch 250/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1904 - val_loss: 1.2814\n",
      "Epoch 251/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1902 - val_loss: 1.2813\n",
      "Epoch 252/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1900 - val_loss: 1.2812\n",
      "Epoch 253/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1899 - val_loss: 1.2810\n",
      "Epoch 254/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.1897 - val_loss: 1.2809\n",
      "Epoch 255/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1896 - val_loss: 1.2807\n",
      "Epoch 256/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1894 - val_loss: 1.2806\n",
      "Epoch 257/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1893 - val_loss: 1.2805\n",
      "Epoch 258/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1891 - val_loss: 1.2804\n",
      "Epoch 259/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1890 - val_loss: 1.2802\n",
      "Epoch 260/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1888 - val_loss: 1.2801\n",
      "Epoch 261/300\n",
      "168/168 [==============================] - 0s 39us/step - loss: 1.1887 - val_loss: 1.2800\n",
      "Epoch 262/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1885 - val_loss: 1.2799\n",
      "Epoch 263/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1884 - val_loss: 1.2798\n",
      "Epoch 264/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1883 - val_loss: 1.2797\n",
      "Epoch 265/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1881 - val_loss: 1.2796\n",
      "Epoch 266/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1863 - val_loss: 1.2792\n",
      "Epoch 267/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1840 - val_loss: 1.2791\n",
      "Epoch 268/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1839 - val_loss: 1.2791\n",
      "Epoch 269/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.1841 - val_loss: 1.2792\n",
      "Epoch 270/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1790 - val_loss: 1.2792\n",
      "Epoch 271/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1788 - val_loss: 1.2793\n",
      "Epoch 272/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1759 - val_loss: 1.2794\n",
      "Epoch 273/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1754 - val_loss: 1.2796\n",
      "Epoch 274/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1754 - val_loss: 1.2797\n",
      "Epoch 275/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1756 - val_loss: 1.2799\n",
      "Epoch 276/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.1758 - val_loss: 1.2801\n",
      "Epoch 277/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1760 - val_loss: 1.2802\n",
      "Epoch 278/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1761 - val_loss: 1.2803\n",
      "Epoch 279/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1762 - val_loss: 1.2804\n",
      "Epoch 280/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1763 - val_loss: 1.2805\n",
      "Epoch 281/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1764 - val_loss: 1.2805\n",
      "Epoch 282/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.1764 - val_loss: 1.2806\n",
      "Epoch 283/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1764 - val_loss: 1.2806\n",
      "Epoch 284/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1763 - val_loss: 1.2806\n",
      "Epoch 285/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1763 - val_loss: 1.2805\n",
      "Epoch 286/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1762 - val_loss: 1.2805\n",
      "Epoch 287/300\n",
      "168/168 [==============================] - 0s 105us/step - loss: 1.1760 - val_loss: 1.2804\n",
      "Epoch 288/300\n",
      "168/168 [==============================] - 0s 27us/step - loss: 1.1759 - val_loss: 1.2804\n",
      "Epoch 289/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1758 - val_loss: 1.2803\n",
      "Epoch 290/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1756 - val_loss: 1.2802\n",
      "Epoch 291/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1754 - val_loss: 1.2801\n",
      "Epoch 292/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1752 - val_loss: 1.2800\n",
      "Epoch 293/300\n",
      "168/168 [==============================] - 0s 93us/step - loss: 1.1750 - val_loss: 1.2800\n",
      "Epoch 294/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1748 - val_loss: 1.2799\n",
      "Epoch 295/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1746 - val_loss: 1.2798\n",
      "Epoch 296/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1744 - val_loss: 1.2797\n",
      "Epoch 297/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1742 - val_loss: 1.2795\n",
      "Epoch 298/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1740 - val_loss: 1.2794\n",
      "Epoch 299/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1738 - val_loss: 1.2793\n",
      "Epoch 300/300\n",
      "168/168 [==============================] - 0s 0us/step - loss: 1.1736 - val_loss: 1.2792\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(\n",
    "                X_train, X_train, \n",
    "                epochs=300,        \n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let plot the Traning and validation Loss in order to see if our model \n",
    "# overfits\n",
    "plt.plot(range(1, len(history.history['loss']) + 1), history.history['loss'],'r', label='Training Loss')\n",
    "plt.plot(range(1, len(history.history['val_loss']) + 1), history.history['val_loss'],'b', label='Validation Loss')\n",
    "plt.title('Training and validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 60        \n",
      "=================================================================\n",
      "Total params: 60\n",
      "Trainable params: 60\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Now we can use the trained encoder/decoder layer of our autoencoder\n",
    "# and build seperated encoder and decoder networks\n",
    "\n",
    "\n",
    "# Using only the first layer of the Autoencoder we can build an encoder model\n",
    "# This model maps an input to its encoded representation\n",
    "# Use again the Keras \"Model\" function passing again the input layer and now \n",
    "# the encoder layer as an output (note that by now the encoder layer is trained)\n",
    "encoder = Model(input_dat, encoded)\n",
    " \n",
    "# Have a look how the model looks like\n",
    "# This is basically the first halb of our autoencoder from above\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 11)                66        \n",
      "=================================================================\n",
      "Total params: 66\n",
      "Trainable params: 66\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Additionally lets build a decoder model that can take an encoded input\n",
    "# and outputs the original variables \n",
    "\n",
    "# create a placeholder for an the encoded input with dimension \n",
    "# equal to the encoding dimension\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "# Get the last layer of the autoencoder model \n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# create the decoder model using the last layer \n",
    "# (the output layer of the autoencoder)\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "# Have a look how the model looks like\n",
    "# This is basically the second half of our autoencoder\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1569071, 1.2051471, 0.       , 0.5601312, 0.       ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see what we can do with these two models...\n",
    "# Lets look at one specific observation (the first observation)\n",
    "encoded_dat = encoder.predict(X_test[[0],:])\n",
    "# This is the encoded data for the first observation (with 8 dimensions)\n",
    "# This does not really have an interpreation but we can see how much\n",
    "# information is in this encoding by using it to reconstruct the \n",
    "# original values using our decoder... (see next cell)\n",
    "encoded_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5673588 ,  0.54881305,  0.58651996,  0.8335578 ,  1.0090089 ,\n",
       "         0.9555786 ,  0.6682779 , -0.82457805,  0.63059926,  0.9481638 ,\n",
       "         1.2009788 ]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the first observation after encoding and then decoding again.\n",
    "# (compare this to the orignal values, next cell...)\n",
    "decoded_dat = decoder.predict(encoded_dat)\n",
    "decoded_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x215fe498088>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets plot a scatter between original input and the \n",
    "# \"predicted\" input after the encoding/decoding steo \n",
    "\n",
    "# If the encoded/decoding steps would work perfectly you would get a \n",
    "# diagonal line.   \n",
    "iCol = 2 # change this to values between (0-27) to plot other variables\n",
    "encoded_dat = encoder.predict(X_test)\n",
    "decoded_dat = decoder.predict(encoded_dat)\n",
    "plt.scatter(X_test[:,iCol],decoded_dat[:,iCol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
